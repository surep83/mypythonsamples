{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surep83/mypythonsamples/blob/main/prompt_chaining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYdIf_WHhRzj"
      },
      "source": [
        "## **Workflow: Prompt chaining**\n",
        "\n",
        "Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one.\n",
        "\n",
        "You can add programmatic checks (see \"gate‚Äù in the diagram below) on any intermediate steps to ensure that the process is still on track.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/BrightPool/udemy-prompt-engineering-course/refs/heads/main/images/prompt-chaining.webp\" alt=\"Agents\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inmAPAPphRzk"
      },
      "source": [
        "## **Use cases:**\n",
        "\n",
        "- Generating Marketing copy, then translating it into a different language.\n",
        "- Writing an outline of a document, checking that the outline meets certain criteria, then writing the document based on the outline.\n",
        "- Using an LLM to clean and standardize raw data, then passing the cleaned data to another LLM for insights, summaries, or visualizations.\n",
        "- Generating a set of detailed questions based on a topic with one LLM, then passing those questions to another LLM to produce well-researched answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80b9iH2GhRzl"
      },
      "outputs": [],
      "source": [
        "%pip install openai pydantic --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1sxm2qAhRzl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0UTIqnrhRzm"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-xxxx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtFhzZSshRzm"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJfE_y2ShRzm"
      },
      "outputs": [],
      "source": [
        "class Story(BaseModel):\n",
        "    story: str\n",
        "    title: str\n",
        "    author: str\n",
        "\n",
        "class StoryPlot(BaseModel):\n",
        "    plot: str\n",
        "\n",
        "class StoryPlots(BaseModel):\n",
        "    plots: List[StoryPlot]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOecgurShRzm"
      },
      "outputs": [],
      "source": [
        "def serial_chain_workflow(topic: str, num_stories: int) -> List[Story]:\n",
        "    \"\"\"Run a serial chain of LLM calls to address the `input_query\"\"\"\n",
        "\n",
        "    stories_plot_prompt = f'''Generate {num_stories} plot ideas for stories about {topic}.\n",
        "    These should be short and concise, and should be suitable for a children's book.\n",
        "    Later on these plot ideas will be used to generate full stories.'''\n",
        "\n",
        "    stories_plot_response = client.beta.chat.completions.parse(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": stories_plot_prompt}],\n",
        "        temperature=0.5,\n",
        "        max_tokens=1000,\n",
        "        response_format=StoryPlots\n",
        "    )\n",
        "\n",
        "    if (stories_plot_response.choices[0].message.parsed.plots == []):\n",
        "        raise ValueError(\"No plots generated\")\n",
        "\n",
        "    print(\"Plots:\")\n",
        "    print(stories_plot_response.choices[0].message.parsed.plots)\n",
        "\n",
        "    # For each story plot, generate a full story\n",
        "    stories = []\n",
        "    for plot in stories_plot_response.choices[0].message.parsed.plots:\n",
        "        story_prompt = f'''Generate a full story based on the following plot: {plot.plot}.\n",
        "        The story should be suitable for a children's book. The title should not be within the story part of the response.\n",
        "        If there is no author, then write \"Unknown\"'''\n",
        "\n",
        "        story_response = client.beta.chat.completions.parse(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": story_prompt}],\n",
        "            temperature=0.5,\n",
        "            max_tokens=1000,\n",
        "            response_format=Story\n",
        "        )\n",
        "\n",
        "        if (story_response.choices[0].message.parsed.story == \"\"):\n",
        "            raise ValueError(\"No story generated\")\n",
        "\n",
        "        stories.append(story_response.choices[0].message.parsed)\n",
        "    return stories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3iirvsIhRzn",
        "outputId": "c179015b-d5d5-4abf-d11e-6f573a8abc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plots:\n",
            "[StoryPlot(plot='**Title: The Magical Data Garden**  \\n**Plot:** In the vibrant town of Numberville, young Ada discovers a secret garden where data grows on trees! Each tree represents different types of data: numbers, shapes, colors, and even sounds. Ada befriends a wise old owl named Professor Hoot, who teaches her how to harvest and use the data to solve puzzles and help her friends. Together, they embark on a journey to unlock the mysteries of the garden, learning the importance of patterns, predictions, and sharing knowledge along the way. The story emphasizes teamwork and curiosity as Ada and her friends use data to make their world a better place.'), StoryPlot(plot=\"**Title: Benny's Big Data Adventure**  \\n**Plot:** Benny the Bear loves playing games on his tablet, but one day, all his games stop working! With the help of Dot, a friendly digital sprite, Benny learns that his games run on data, and it's up to him to fix it. Dot leads Benny on a thrilling adventure through Data Land, where they meet quirky characters like the Sorting Squirrels and the Graph Giraffes. Each character teaches Benny a new data skill, from organizing information to understanding charts. As Benny learns, he gains the tools to fix his games and discovers the magic of data science, all while having fun and making new friends.\")]\n"
          ]
        }
      ],
      "source": [
        "stories = serial_chain_workflow(\"data science\", 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72JBfGcfhRzn",
        "outputId": "9aac4a72-d910-4cfe-f63b-031b2246cac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Magical Data Adventure\n",
            "[Your Name]\n"
          ]
        }
      ],
      "source": [
        "print(stories[0].title)\n",
        "print(stories[0].author)"
      ]
    }
  ]
}